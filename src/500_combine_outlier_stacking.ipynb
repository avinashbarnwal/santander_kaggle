{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn  as sns\n",
    "import lightgbm as lgb\n",
    "from   sklearn.linear_model    import BayesianRidge\n",
    "from   sklearn.model_selection import StratifiedKFold,RepeatedKFold\n",
    "from   sklearn.metrics         import mean_squared_error,precision_recall_curve,f1_score,roc_curve,auc\n",
    "from   sklearn.ensemble        import RandomTreesEmbedding\n",
    "from   scipy.stats             import ks_2samp\n",
    "import utils\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Idea\n",
    "# Two models for stacking using Stratified  + Repeated (LGB)\n",
    "# Create prediction overall\n",
    "# Two models for stacking using Stratified  + Repeated (LGB) without outlier\n",
    "# Create outlier model\n",
    "# For non-outlier use step 3 model\n",
    "# For outlier use step 1 model\n",
    "\n",
    "\n",
    "# This is different compared to others.\n",
    "# Log Transformation \n",
    "# (np.log1p) and an exponential function (np.expm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train                 = pd.read_pickle(\"../data/input/train_test/train_final.pkl\")\n",
    "df_test                  = pd.read_pickle(\"../data/input/train_test/test_final.pkl\")\n",
    "\n",
    "significant_features     = utils.load_obj(\"significant_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "train_columns = [c for c in train_columns    if c in significant_features]\n",
    "\n",
    "X_train       = df_train[train_columns]\n",
    "target        = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_different_features(df_train,df_test,train_columns):\n",
    "    \n",
    "    list_p_value =[]\n",
    "    new_train_columns = []\n",
    "    for i in train_columns:\n",
    "        if df_test[i].dtypes != 'object':\n",
    "            new_train_columns.append(i)\n",
    "            list_p_value.append(ks_2samp(df_test[i] , df_train[i])[1])\n",
    "\n",
    "    Se = pd.Series(list_p_value, index = new_train_columns).sort_values() \n",
    "    list_discarded = list(Se[Se < .1].index)\n",
    "    \n",
    "    return list_discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_discarded = discard_different_features(df_train,df_test,train_columns)\n",
    "train_columns    = [c for c in train_columns if c not in column_discarded] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(df_train,df_test,target,param,train_columns,type_fold = \"SKFold\",n_splits=5,df_train_svd=None,df_test_svd=None,SVD=True):\n",
    "    \n",
    "    if type_fold==\"SKFold\":\n",
    "        folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "    if type_fold==\"RepeatFold\":\n",
    "        folds = RepeatedKFold(n_splits=n_splits, n_repeats=2, random_state=0)\n",
    "             \n",
    "    oof                   = np.zeros(len(df_train))\n",
    "    predictions           = np.zeros(len(df_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    cv_loss               = 0    \n",
    "    num_round             = 10000    \n",
    "    \n",
    "    if SVD == False:\n",
    "        for fold_, (train_idx, valid_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n",
    "\n",
    "            #param = {**param,**extra}\n",
    "\n",
    "            print(\"fold {}\".format(fold_))\n",
    "\n",
    "            train_data     = lgb.Dataset(df_train.iloc[train_idx][train_columns], label=target.iloc[train_idx])#, categorical_feature=categorical_feats)\n",
    "            valid_data     = lgb.Dataset(df_train.iloc[valid_idx][train_columns], label=target.iloc[valid_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "            clf            = lgb.train(param,train_data,num_round,valid_sets = [train_data, valid_data],verbose_eval=-1,early_stopping_rounds = 100)\n",
    "\n",
    "            oof[valid_idx] = clf.predict(df_train.iloc[valid_idx][train_columns], num_iteration=clf.best_iteration)\n",
    "\n",
    "            fold_importance_df                 = pd.DataFrame()\n",
    "            fold_importance_df[\"Feature\"]      = train_columns\n",
    "            fold_importance_df[\"importance\"]   = clf.feature_importance()\n",
    "            fold_importance_df[\"fold\"]         = fold_ + 1\n",
    "            feature_importance_df              = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "            if type_fold==\"SKFold\":\n",
    "                predictions += clf.predict(df_test[train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "                cv_loss     += np.sqrt(mean_squared_error(oof, target))\n",
    "\n",
    "            if type_fold==\"RepeatFold\":\n",
    "                predictions += clf.predict(df_test[train_columns], num_iteration=clf.best_iteration)/10\n",
    "                cv_loss     += np.sqrt(mean_squared_error(oof, target))    \n",
    "\n",
    "            #print(\"Cumulative CV Loss = \",cv_loss/(fold_+1))\n",
    "    else:\n",
    "        for fold_, (train_idx, valid_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n",
    "\n",
    "            #param = {**param,**extra}\n",
    "\n",
    "            print(\"fold {}\".format(fold_))\n",
    "\n",
    "            train_data     = lgb.Dataset(df_train.iloc[train_idx][train_columns], label=target.iloc[train_idx])#, categorical_feature=categorical_feats)\n",
    "            valid_data     = lgb.Dataset(df_train.iloc[valid_idx][train_columns], label=target.iloc[valid_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "            clf            = lgb.train(param,train_data,num_round,valid_sets = [train_data, valid_data],verbose_eval=-1,early_stopping_rounds = 100)\n",
    "\n",
    "            oof[valid_idx] = clf.predict(df_train.iloc[valid_idx][train_columns], num_iteration=clf.best_iteration)\n",
    "\n",
    "            fold_importance_df                 = pd.DataFrame()\n",
    "            fold_importance_df[\"Feature\"]      = train_columns\n",
    "            fold_importance_df[\"importance\"]   = clf.feature_importance()\n",
    "            fold_importance_df[\"fold\"]         = fold_ + 1\n",
    "            feature_importance_df              = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "            if type_fold==\"SKFold\":\n",
    "                predictions += clf.predict(df_test[train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "                cv_loss     += np.sqrt(mean_squared_error(oof, target))\n",
    "\n",
    "            if type_fold==\"RepeatFold\":\n",
    "                predictions += clf.predict(df_test[train_columns], num_iteration=clf.best_iteration)/10\n",
    "                cv_loss     += np.sqrt(mean_squared_error(oof, target))    \n",
    "\n",
    "            #print(\"Cumulative CV Loss = \",cv_loss/(fold_+1))\n",
    "\n",
    "    feature_importance_df = feature_importance_df.groupby(['Feature'])['importance'].mean().reset_index()\n",
    "    CV_LOSS = np.sqrt(mean_squared_error(oof, target))\n",
    "    print(\"CV Loss = \",CV_LOSS)    \n",
    "    \n",
    "    return predictions,oof,feature_importance_df,CV_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(train_stack1,train_stack2,prediction_stack1,prediction_stack2,target):\n",
    "\n",
    "    train_stack = np.vstack([train_stack1,train_stack2]).transpose()\n",
    "    test_stack  = np.vstack([prediction_stack1,prediction_stack2]).transpose()\n",
    "\n",
    "    #Concatenate Side ways\n",
    "\n",
    "    folds       = RepeatedKFold(n_splits=5,n_repeats=1,random_state=0)\n",
    "    oof_stack   = np.zeros(train_stack.shape[0])\n",
    "    predictions_stack = np.zeros(test_stack.shape[0])\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, target)):\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "        trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "        val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_) + \"-\" * 10)\n",
    "        clf = BayesianRidge()\n",
    "        clf.fit(trn_data, trn_y)\n",
    "    \n",
    "        oof_stack[val_idx] = clf.predict(val_data)\n",
    "        predictions_stack += clf.predict(test_stack)/5\n",
    "\n",
    "    print(np.sqrt(mean_squared_error(target.values, oof_stack)))\n",
    "    \n",
    "    return predictions_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_model_training(df_train,df_test,train_columns,type_fold=\"SKFold\",n_splits=5):\n",
    "    \n",
    "    target = df_train['outliers']\n",
    "    \n",
    "    #param  = {'num_leaves': 40,\n",
    "    #         'min_data_in_leaf': 30, \n",
    "    #         'objective':'binary',\n",
    "    #         'max_depth': 6,\n",
    "    #         'learning_rate': 0.015,\n",
    "    #         \"min_child_samples\": 20,\n",
    "    #         \"boosting_type\": \"gbdt\",\n",
    "    #         \"feature_fraction\": 0.6,\n",
    "    #         \"bagging_freq\": 1,\n",
    "    #         \"bagging_fraction\": 0.8 ,\n",
    "    #         \"bagging_seed\": 11,\n",
    "    #         \"metric\": 'binary_logloss',\n",
    "    #         \"lambda_l1\": 0.1,\n",
    "    #         \"verbosity\": -1,\n",
    "    #         \"nthread\": 4,\n",
    "    #         \"n_estimators\":400,\n",
    "    #         \"random_state\": 0}\n",
    "    \n",
    "    #     #param = {\n",
    "    #             'objective':'binary',\n",
    "    #             \"boosting\" : \"gbdt\",\n",
    "    #             'num_leaves': 22,\n",
    "    #             'max_depth': 8,\n",
    "    #             \"min_child_weight\" : 59.03,\n",
    "    #             'min_data_in_leaf': 48, \n",
    "    #             \"subsample\" : 0.8392 ,\n",
    "    #             \"subsample_freq\" : 19, \n",
    "    #             \"colsample_bytree\" : 0.8727,\n",
    "    #             \"reg_alpha\":  8.395,\n",
    "    #             \"reg_lambda\" : 37.98,\n",
    "    #             \"n_estimators\" : 452,\n",
    "    #             'learning_rate': 0.05846,\n",
    "    #             \"metric\": 'binary_logloss',            \n",
    "    #             \"verbosity\": -1,\n",
    "    #             'random_state' : 0\n",
    "    #         }\n",
    "\n",
    "    #     param = {\n",
    "    #             'objective':'binary',\n",
    "    #             \"boosting\" : \"gbdt\",\n",
    "    #             \"metric\": 'binary_logloss',  \n",
    "    #             'num_leaves': 22,\n",
    "    #             'max_depth': 8,\n",
    "    #             'min_child_weight': 4.031922049091266,\n",
    "    #             'subsample':        0.4949576665555243,\n",
    "    #             'subsample_freq':   3 ,\n",
    "    #             'colsample_bytree': 0.4966523480028763,\n",
    "    #             'reg_alpha':        0.0010578009043560321,\n",
    "    #             'reg_lambda':       3.8551470327971784,\n",
    "    #             'n_estimators':     995.082438933931,\n",
    "    #             'learning_rate':    0.30615518606160763,\n",
    "    #             'min_data_in_leaf': 12,\n",
    "    #             \"verbosity\":        -1,\n",
    "    #             'random_state' :    0\n",
    "    #             }\n",
    "    \n",
    "    #     param = {\n",
    "    #                 'objective':         'binary',\n",
    "    #                 \"boosting\" :         \"gbdt\",\n",
    "    #                 \"metric\"   :         'binary_logloss',  \n",
    "    #                 'num_leaves':        int(70.47972983975363),\n",
    "    #                 'max_depth':         int(8.392477757408827),\n",
    "    #                 'min_child_weight':  59.60535124223574,\n",
    "    #                 'subsample':         0.7951795353622787,\n",
    "    #                 'subsample_freq':    int(round(8.596699743076439,0)),\n",
    "    #                 'colsample_bytree':  0.945243324411453,\n",
    "    #                 'reg_alpha':         0.7675451834724589,\n",
    "    #                 'reg_lambda':        9.14907461407702,\n",
    "    #                 'n_estimators':      int(907.6248184257204),\n",
    "    #                 'learning_rate':     0.369583706630833,\n",
    "    #                 'min_data_in_leaf':  int(6.4831756298597085)}\n",
    "    \n",
    "    param = {\n",
    "            'objective':         'binary',\n",
    "            \"boosting\" :         \"gbdt\",\n",
    "            \"metric\"   :         'binary_logloss',  \n",
    "            'num_leaves':        int(round(23.139822370824557,0)),\n",
    "            'max_depth':         int(round(8.58975375654246,0)),\n",
    "            'min_child_weight':  2.7078843623515434,\n",
    "            'subsample':         0.45759536703648757,\n",
    "            'subsample_freq':    int(round(15.291177856629368,0)),\n",
    "            'colsample_bytree':  0.48711283395859273,\n",
    "            'reg_alpha':         0.4370945570821483,\n",
    "            'reg_lambda':        1.8421506357456277,\n",
    "            'n_estimators':      int(round(769.7003729230836,0)),\n",
    "            'learning_rate':     0.3824725368523456,\n",
    "            'min_data_in_leaf':  int(round(9.218152218008113,0)),\n",
    "            'random_state':       0\n",
    "    }\n",
    "    \n",
    "    predictions,oof,feature_importance_df = model_training(df_train,df_test,target,param,train_columns,type_fold=type_fold,n_splits=5)\n",
    "    \n",
    "    return predictions,oof "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_outlier_training(df_train,df_test,train_columns,type_fold=\"SKFold\"):\n",
    "\n",
    "    df_train      = df_train[df_train['outliers'] == 0]\n",
    "    target        = df_train['target']\n",
    "    \n",
    "     # params optimized by optuna\n",
    "#     param ={\n",
    "#        'task':       'train',\n",
    "#        'boosting':   'goss',\n",
    "#        'objective':  'regression',\n",
    "#        'metric':     'rmse',\n",
    "#        'learning_rate': 0.01,\n",
    "#        'subsample': 0.9855232997390695,\n",
    "#        'max_depth': 7,\n",
    "#        'top_rate': 0.9064148448434349,\n",
    "#        'num_leaves': 63,\n",
    "#        'min_child_weight': 41.9612869171337,\n",
    "#        'other_rate': 0.0721768246018207,\n",
    "#        'reg_alpha': 9.677537745007898,\n",
    "#        'colsample_bytree': 0.5665320670155495,\n",
    "#        'min_split_gain': 9.820197773625843,\n",
    "#        'reg_lambda': 8.2532317400459,\n",
    "#        'min_data_in_leaf': 21,\n",
    "#        'verbose': -1}\n",
    "    \n",
    "#Version 1    \n",
    "#     param = {\n",
    "#               'objective':         'regression',\n",
    "#               \"boosting\":          \"gbdt\",\n",
    "#               'colsample_bytree':  0.7742624484824272,\n",
    "#               'learning_rate':     0.025686010236442722,\n",
    "#               'max_depth':         int(8.501720245641943),\n",
    "#               'min_child_weight':  59.6919305864503,\n",
    "#               'min_data_in_leaf':  int(13.190424020982828),\n",
    "#               'n_estimators':      int(362.55733945351284),\n",
    "#               'num_leaves':        int(78.15469597634271),\n",
    "#               'reg_alpha':         9.785115624171436,\n",
    "#               'reg_lambda':        2.7562911363915816,\n",
    "#               'subsample':         0.7835388457721937,\n",
    "#               'subsample_freq':    int(4.022277822542142),\n",
    "#               \"metric\"           : 'rmse',            \n",
    "#               \"verbosity\"        : -1,\n",
    "#               'random_state'     : 0\n",
    "#         }\n",
    "\n",
    "#Version 2 \n",
    "#     param = {\n",
    "#         'objective':         'regression',\n",
    "#         \"boosting\":          'gbdt',\n",
    "#         'num_leaves':        int(12.193051675829718),\n",
    "#          'max_depth':        int(round(7.716985651169187,0)),\n",
    "#          'min_child_weight': 29.72335065298584,\n",
    "#          'subsample':        0.805638808022076,\n",
    "#          'subsample_freq':   int(1.1682326885196326),\n",
    "#          'colsample_bytree': 0.43199192892339977,\n",
    "#          'reg_alpha':        2.4746685205728824,\n",
    "#          'reg_lambda':       10.964003773164277,\n",
    "#          'n_estimators':     int(round(999.9239652223674,0)),\n",
    "#          'learning_rate':    0.03808021689360447,\n",
    "#          'min_data_in_leaf': int(44.326079777911545),\n",
    "#          \"metric\"           : 'rmse',            \n",
    "#          \"verbosity\"        : -1,\n",
    "#          'random_state'     : 0}\n",
    "\n",
    "#Version 3\n",
    "    param = {\n",
    "            'objective':          'regression',\n",
    "            \"boosting\":           'gbdt',\n",
    "            'num_leaves':         int(round(69.73996492517938,0)),\n",
    "            'max_depth':          int(5.032821749635573),\n",
    "            'min_child_weight':   58.72169981385353,\n",
    "            'subsample':          0.7743352484347494,\n",
    "            'subsample_freq':     int(round(10.531453984618025,0)),\n",
    "            'colsample_bytree':   0.6079607673654336,\n",
    "            'reg_alpha':          1.2516468359917554,\n",
    "            'reg_lambda':         39.91422683261549,\n",
    "            'n_estimators':       int(round(984.4508643679893,0)),\n",
    "            'learning_rate':      0.04440594033746144,\n",
    "            'min_data_in_leaf':   int(round(11.700552265707564,0)),\n",
    "            \"metric\"           :  'rmse',            \n",
    "            \"verbosity\"        :  -1,\n",
    "            'random_state'     :  0\n",
    "            }\n",
    "\n",
    "\n",
    "    predictions,oof,feature_importance_df = model_training(df_train,df_test,target,param,train_columns,type_fold=type_fold,n_splits=5)\n",
    "    \n",
    "    return predictions,oof,feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_regression_training(df_train,df_test,train_columns,type_fold=\"SKFold\"):\n",
    "\n",
    "    target = df_train['target']\n",
    "    \n",
    "     # params optimized by optuna\n",
    "#     param ={\n",
    "#        'task':       'train',\n",
    "#        'boosting':   'goss',\n",
    "#        'objective':  'regression',\n",
    "#        'metric':     'rmse',\n",
    "#        'learning_rate': 0.01,\n",
    "#        'subsample': 0.9855232997390695,\n",
    "#        'max_depth': 7,\n",
    "#        'top_rate': 0.9064148448434349,\n",
    "#        'num_leaves': 63,\n",
    "#        'min_child_weight': 41.9612869171337,\n",
    "#        'other_rate': 0.0721768246018207,\n",
    "#        'reg_alpha': 9.677537745007898,\n",
    "#        'colsample_bytree': 0.5665320670155495,\n",
    "#        'min_split_gain': 9.820197773625843,\n",
    "#        'reg_lambda': 8.2532317400459,\n",
    "#        'min_data_in_leaf': 21,\n",
    "#        'verbose': -1}\n",
    "    \n",
    "    param = {\n",
    "              'objective':         'regression',\n",
    "              \"boosting\":          \"gbdt\",\n",
    "              'colsample_bytree':  0.7742624484824272,\n",
    "              'learning_rate':     0.025686010236442722,\n",
    "              'max_depth':         int(8.501720245641943),\n",
    "              'min_child_weight':  59.6919305864503,\n",
    "              'min_data_in_leaf':  int(13.190424020982828),\n",
    "              'n_estimators':      int(362.55733945351284),\n",
    "              'num_leaves':        int(78.15469597634271),\n",
    "              'reg_alpha':         9.785115624171436,\n",
    "              'reg_lambda':        2.7562911363915816,\n",
    "              'subsample':         0.7835388457721937,\n",
    "              'subsample_freq':    int(4.022277822542142),\n",
    "              \"metric\"           : 'rmse',            \n",
    "              \"verbosity\"        : -1,\n",
    "              'random_state'     : 0\n",
    "        }\n",
    "\n",
    "    predictions,oof,feature_importance_df = model_training(df_train,df_test,target,param,train_columns,type_fold=type_fold,n_splits=5)\n",
    "    \n",
    "    return predictions,oof,feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_outlier_non_outlier(best_submission,outlier_id,model_without_outliers,final_submission):\n",
    "    \n",
    "    path                       = \"../result/\"\n",
    "    file_name                  = best_submission + \".csv\"\n",
    "    best_submission            = pd.read_csv(path + file_name)\n",
    "    outlier_id['outlier_flag'] = 1\n",
    "\n",
    "    most_likely_liers          = best_submission.merge(outlier_id,how='right')\n",
    "    model_without_outliers.rename(columns = {'target':'target_non_outlier'},inplace=True)   \n",
    "    model_without_outliers = model_without_outliers.merge(most_likely_liers,how='left') \n",
    "\n",
    "    model_without_outliers['new_target'] = model_without_outliers.apply(lambda x: x['target'] if x['outlier_flag'] == 1.0 else x['target_non_outlier'],axis=1)\n",
    "    model_without_outliers = model_without_outliers[['card_id','new_target']]\n",
    "    model_without_outliers = model_without_outliers.rename(columns={'new_target':'target'})\n",
    "        \n",
    "    file_name = final_submission + \".csv\"\n",
    "    \n",
    "    model_without_outliers.to_csv(path+file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to hold cv results\n",
    "\n",
    "#Version 1\n",
    "\n",
    "#param = {'num_leaves': 40,\n",
    "#         'min_data_in_leaf': 30, \n",
    "#         'objective':'regression',\n",
    "#         'max_depth': 6,\n",
    "#         'learning_rate': 0.015,\n",
    "#         \"min_child_samples\": 20,\n",
    "#         \"boosting_type\": \"gbdt\",\n",
    "#         \"feature_fraction\": 0.6,\n",
    "#         \"bagging_freq\": 1,\n",
    "#         \"bagging_fraction\": 0.8 ,\n",
    "#         \"bagging_seed\": 11,\n",
    "#         \"metric\": 'rmse',\n",
    "#         \"lambda_l1\": 0.1,\n",
    "#         \"verbosity\": -1,\n",
    "#         \"nthread\": 4,\n",
    "#         \"n_estimators\": 400,\n",
    "#         \"random_state\": 0}\n",
    "\n",
    "# Version 2\n",
    "# param = {\n",
    "#             'objective':'regression',\n",
    "#             \"boosting\": \"gbdt\",\n",
    "#             'num_leaves'       : 22,\n",
    "#             'max_depth'        : 8,\n",
    "#             \"min_child_weight\" : 59.03,\n",
    "#             'min_data_in_leaf' : 48, \n",
    "#             \"subsample\"        : 0.8392,\n",
    "#             \"subsample_freq\"   : 19,\n",
    "#             \"colsample_bytree\" : 0.8727,\n",
    "#             \"reg_alpha\"        : 8.395,\n",
    "#             \"reg_lambda\"       : 37.98,\n",
    "#             \"n_estimators\"     : 452,\n",
    "#             'learning_rate'    : 0.01,\n",
    "#             \"metric\"           : 'rmse',            \n",
    "#             \"verbosity\"        : -1,\n",
    "#             'random_state'     : 0\n",
    "#         }\n",
    "\n",
    "# Version 3\n",
    "# param = {\n",
    "#          'objective':         'regression',\n",
    "#          \"boosting\":          \"gbdt\",\n",
    "#          'colsample_bytree':  0.7742624484824272,\n",
    "#          'learning_rate':     0.025686010236442722,\n",
    "#          'max_depth':         int(8.501720245641943),\n",
    "#          'min_child_weight':  59.6919305864503,\n",
    "#          'min_data_in_leaf':  int(13.190424020982828),\n",
    "#          'n_estimators':      int(362.55733945351284),\n",
    "#          'num_leaves':        int(78.15469597634271),\n",
    "#          'reg_alpha':         9.785115624171436,\n",
    "#          'reg_lambda':        2.7562911363915816,\n",
    "#          'subsample':         0.7835388457721937,\n",
    "#          'subsample_freq':    int(4.022277822542142),\n",
    "#          \"metric\"           : 'rmse',            \n",
    "#          \"verbosity\"        : -1,\n",
    "#          'random_state'     : 1\n",
    "#         }\n",
    "\n",
    "\n",
    "# Version 4\n",
    "# param = {\n",
    "#          'objective':         'regression',\n",
    "#          \"boosting\":          \"gbdt\",\n",
    "#          'colsample_bytree':  0.7742624484824272,\n",
    "#          'learning_rate':     0.025686010236442722,\n",
    "#          'max_depth':         int(8.501720245641943),\n",
    "#          'min_child_weight':  59.6919305864503,\n",
    "#          'min_data_in_leaf':  int(13.190424020982828),\n",
    "#          'n_estimators':      int(362.55733945351284),\n",
    "#          'num_leaves':        int(78.15469597634271),\n",
    "#          'reg_alpha':         9.785115624171436,\n",
    "#          'reg_lambda':        2.7562911363915816,\n",
    "#          'subsample':         0.7835388457721937,\n",
    "#          'subsample_freq':    int(4.022277822542142),\n",
    "#          \"metric\"           : 'rmse',            \n",
    "#          \"verbosity\"        : -1,\n",
    "#          'random_state'     : 1\n",
    "#         }\n",
    "\n",
    "#Version 5\n",
    "# param = {\n",
    "#          'objective':'regression',\n",
    "#          \"boosting\": \"gbdt\",            \n",
    "#          \"metric\": 'rmse',            \n",
    "#          \"verbosity\": -1,\n",
    "#          'num_leaves': int(round(78.8550248827644,0)),\n",
    "#          'max_depth': int(round(4.364837032978728,0)),\n",
    "#          'min_child_weight': 2.510726551117959,\n",
    "#          'subsample': 0.6195251212654296,\n",
    "#          'subsample_freq': int(round(1.3405825234524915,0)),\n",
    "#          'colsample_bytree': 0.5400026592586313,\n",
    "#          'reg_alpha': 6.834814893383772,\n",
    "#          'reg_lambda': 35.35979905337201,\n",
    "#          'n_estimators': int(round(985.0790468368917,0)),\n",
    "#          'learning_rate': 0.19687878426596275,\n",
    "#          'min_data_in_leaf': int(round(5.372504907365297,0)),\n",
    "#          'random_state' : 0}\n",
    "\n",
    "\n",
    "#Version 6\n",
    "# param = {\n",
    "#         'objective':         'regression',\n",
    "#         \"boosting\":          'gbdt',\n",
    "#         'num_leaves':        int(12.193051675829718),\n",
    "#          'max_depth':        int(round(7.716985651169187,0)),\n",
    "#          'min_child_weight': 29.72335065298584,\n",
    "#          'subsample':        0.805638808022076,\n",
    "#          'subsample_freq':   int(1.1682326885196326),\n",
    "#          'colsample_bytree': 0.43199192892339977,\n",
    "#          'reg_alpha':        2.4746685205728824,\n",
    "#          'reg_lambda':       10.964003773164277,\n",
    "#          'n_estimators':     int(round(999.9239652223674,0)),\n",
    "#          'learning_rate':    0.03808021689360447,\n",
    "#          'min_data_in_leaf': int(44.326079777911545),\n",
    "#          \"metric\"           : 'rmse',            \n",
    "#          \"verbosity\"        : -1,\n",
    "#          'random_state'     : 0}\n",
    "\n",
    "#Version 7\n",
    "# param = {\n",
    "#             'objective':          'regression',\n",
    "#             \"boosting\":           'gbdt',\n",
    "#             'num_leaves':         int(round(69.73996492517938,0)),\n",
    "#             'max_depth':          int(5.032821749635573),\n",
    "#             'min_child_weight':   58.72169981385353,\n",
    "#             'subsample':          0.7743352484347494,\n",
    "#             'subsample_freq':     int(round(10.531453984618025,0)),\n",
    "#             'colsample_bytree':   0.6079607673654336,\n",
    "#             'reg_alpha':          1.2516468359917554,\n",
    "#             'reg_lambda':         39.91422683261549,\n",
    "#             'n_estimators':       int(round(984.4508643679893,0)),\n",
    "#             'learning_rate':      0.04440594033746144,\n",
    "#             'min_data_in_leaf':   int(round(11.700552265707564,0)),\n",
    "#             \"metric\":             'rmse',            \n",
    "#             \"verbosity\":          -1,\n",
    "#             'random_state':       0\n",
    "#     }\n",
    "\n",
    "# params optimized by optuna - 3.691\n",
    "# param ={'task':        'train',\n",
    "#        'boosting':     'goss',\n",
    "#        'objective':    'regression',\n",
    "#        'metric':       'rmse',\n",
    "#        'learning_rate': 0.01,\n",
    "#        'subsample':     0.9855232997390695,\n",
    "#        'max_depth':     7,\n",
    "#        'top_rate':      0.9064148448434349,\n",
    "#        'num_leaves':    63,\n",
    "#        'min_child_weight': 41.9612869171337,\n",
    "#        'other_rate':    0.0721768246018207,\n",
    "#        'reg_alpha':     9.677537745007898,\n",
    "#        'colsample_bytree': 0.5665320670155495,\n",
    "#        'min_split_gain': 9.820197773625843,\n",
    "#        'reg_lambda': 8.2532317400459,\n",
    "#        'min_data_in_leaf': 21,\n",
    "#        'verbose': -1}\n",
    "\n",
    "#Params optimized - 3.695\n",
    "#param = {'num_leaves': 51,\n",
    "#          'min_data_in_leaf': 35, \n",
    "#          'objective':'regression',\n",
    "#          'max_depth': -1,\n",
    "#          'learning_rate': 0.008,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.85,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.82,\n",
    "#          \"bagging_seed\": 42,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"lambda_l1\": 0.11,\n",
    "#          \"verbosity\": -1,\n",
    "#          \"nthread\": 4,\n",
    "#          \"random_state\": 2019}\n",
    "\n",
    "#Version 8##\n",
    "param = {\n",
    "        'objective':        'regression',\n",
    "        'boosting':         'gbdt',\n",
    "        'num_leaves':       351,\n",
    "        'max_depth':        12,\n",
    "        'min_child_weight': 4.081862709896796,\n",
    "        'subsample':        0.8099691929098813,\n",
    "        'subsample_freq':   31,\n",
    "        'colsample_bytree': 0.09377558631763243,\n",
    "        'reg_alpha':        0.0002916032212299976,\n",
    "        'reg_lambda':       3.0959773794206487,\n",
    "        'n_estimators':     1393,\n",
    "        'learning_rate':    0.005958169006526415,\n",
    "        'min_data_in_leaf': 22,\n",
    "        'metric':           'rmse',\n",
    "        'verbose':          -1,\n",
    "        'random_state' :    0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1246]\ttraining's rmse: 2.84648\tvalid_1's rmse: 3.65827\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1093]\ttraining's rmse: 2.88617\tvalid_1's rmse: 3.67574\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1061]\ttraining's rmse: 2.90978\tvalid_1's rmse: 3.64267\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1180]\ttraining's rmse: 2.87392\tvalid_1's rmse: 3.64952\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[901]\ttraining's rmse: 2.97504\tvalid_1's rmse: 3.63933\n",
      "CV Loss =  3.653128692204508\n"
     ]
    }
   ],
   "source": [
    "predictions_skfold,oof_skfold,feature_importance_skfold,CV_LOSS = model_training(df_train,df_test,target,param,train_columns,type_fold = \"SKFold\",n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature_importance_skfold_summ =feature_importance_skfold.groupby(['Feature'])['importance'].sum().reset_index()\n",
    "#feature_importance_skfold_summ.to_csv(\"feature_importance_skfold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "X   = df_train[train_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=2, n_iter=7,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_svd  = svd.transform(df_test[train_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_svd =  svd.transform(df_train[train_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      2 ... 201913 201914 201916]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[526]\ttraining's rmse: 3.83302\tvalid_1's rmse: 3.85504\n",
      "[     1      2      3 ... 201913 201914 201915]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[526]\ttraining's rmse: 3.83532\tvalid_1's rmse: 3.84689\n",
      "[     0      1      2 ... 201913 201915 201916]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[217]\ttraining's rmse: 3.84157\tvalid_1's rmse: 3.83716\n",
      "[     0      2      3 ... 201914 201915 201916]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[526]\ttraining's rmse: 3.83685\tvalid_1's rmse: 3.8426\n",
      "[     0      1      3 ... 201914 201915 201916]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[620]\ttraining's rmse: 3.8377\tvalid_1's rmse: 3.83628\n"
     ]
    }
   ],
   "source": [
    "for fold_, (train_idx, valid_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n",
    "    \n",
    "    print(train_idx)\n",
    "    \n",
    "    train_data     = lgb.Dataset(df_train_svd[train_idx], label=target.iloc[train_idx])#, categorical_feature=categorical_feats)\n",
    "    valid_data     = lgb.Dataset(df_train_svd[valid_idx], label=target.iloc[valid_idx])#, categorical_feature=categorical_feats)\n",
    "    num_round      = 10000\n",
    "    clf            = lgb.train(param,train_data,num_round,valid_sets = [train_data, valid_data],verbose_eval=-1,early_stopping_rounds = 100)\n",
    "        \n",
    "    #oof[valid_idx] = clf.predict(df_train_svd[valid_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_append(Name,CV_LOSS,param,model_type):\n",
    "    \n",
    "    path = '../data/output/'\n",
    "    \n",
    "    try:\n",
    "        file_name         = path + \"result.json\"\n",
    "        result            = pd.read_json(file_name)\n",
    "        row,col           = result.shape\n",
    "        result.loc[row,:] = [CV_LOSS,Name,model_type,param]\n",
    "        result.to_json(\"../data/output/result.json\")\n",
    "        \n",
    "    except:    \n",
    "        result            = pd.DataFrame(columns=['CV_LOSS','Name','model_type','params'])\n",
    "        result.loc[0,:]   = [CV_LOSS,Name,model_type,param]\n",
    "        result.to_json(\"../data/output/result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/output/'\n",
    "file_name         = path + \"result.json\"\n",
    "result            = pd.read_json(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_append(\"Version_3\",CV_LOSS,param,\"non-process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_feature_importance(data):\n",
    "    data.to_csv(\"../data/output/feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_feature_importance(feature_importance_skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_features(data,K=10):\n",
    "    \n",
    "    percentile          = np.percentile(feature_importance_skfold['importance'], np.arange(0, 100, 10)) # deciles\n",
    "    selected_percentile = percentile[int(K/10)]\n",
    "    selected_feature    = feature_importance_skfold[feature_importance_skfold['importance'] >= selected_percentile]\n",
    "    \n",
    "    return selected_feature['Feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns       = list(select_important_features(feature_importance_skfold['importance'],10))\n",
    "#multi_y    = list(map(lambda x : 0 if x >= percentile[0] and x< percentile[1] else 1 if x >= percentile[1] and x< percentile[2] else 2 if x >= percentile[2] and x< percentile[3] else 3,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1198]\ttraining's rmse: 2.85754\tvalid_1's rmse: 3.65776\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[986]\ttraining's rmse: 2.93173\tvalid_1's rmse: 3.67602\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[969]\ttraining's rmse: 2.95054\tvalid_1's rmse: 3.64536\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1231]\ttraining's rmse: 2.85453\tvalid_1's rmse: 3.65361\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1026]\ttraining's rmse: 2.9243\tvalid_1's rmse: 3.64505\n",
      "CV Loss =  3.6555766051066465\n"
     ]
    }
   ],
   "source": [
    "predictions_skfold,oof_skfold,feature_importance_skfold,CV_LOSS = model_training(df_train,df_test,target,param,train_columns,type_fold = \"SKFold\",n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_append(\"Version_4\",CV_LOSS,param,\"non-process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's binary_logloss: 0.0413834\tvalid_1's binary_logloss: 0.048096\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's binary_logloss: 0.0431967\tvalid_1's binary_logloss: 0.0489456\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's binary_logloss: 0.0430617\tvalid_1's binary_logloss: 0.0471646\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's binary_logloss: 0.0444169\tvalid_1's binary_logloss: 0.0476139\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's binary_logloss: 0.0445825\tvalid_1's binary_logloss: 0.0480626\n",
      "CV Loss =  0.10231736255089778\n"
     ]
    }
   ],
   "source": [
    "prediction_outlier_skfold,oof_outlier_skfold = outlier_model_training(df_train,df_test,train_columns,type_fold=\"SKFold\",n_splits=5)\n",
    "# In case missing some predictable outlier, we choose top 25000 with highest outliers likelyhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_outlier_data(df_train,oof_outlier_skfold):\n",
    "    \n",
    "    df_train_outlier                      = pd.DataFrame({\"card_id\":df_train['card_id'].values})\n",
    "    df_train_outlier['prob']              = oof_outlier_skfold\n",
    "    df_train_outlier['outlier']           = df_train['outliers']\n",
    "    fpr, tpr, thresholds_tpr              = roc_curve(df_train_outlier['outlier'],df_train_outlier['prob'])\n",
    "    auc_result                            = auc(fpr, tpr)\n",
    "    i                                     = np.arange(len(tpr)) \n",
    "    roc                                   = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(thresholds_tpr, index=i)})\n",
    "    roc_t                                 = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n",
    "    df_train_outlier['predicted_outlier'] = df_train_outlier['prob'].map(lambda x: 1 if x>=roc_t['threshold'].values[0] else 0)\n",
    "    df_train['predicted_outlier']         = df_train_outlier['predicted_outlier']\n",
    "    df_train_non_outlier                  = df_train[df_train['predicted_outlier'] == 0]\n",
    "    df_train_outlier                      = df_train[df_train['predicted_outlier'] == 1]\n",
    "    return df_train_non_outlier,df_train_outlier,roc_t['threshold'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[637]\ttraining's rmse: 1.48565\tvalid_1's rmse: 1.57717\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[551]\ttraining's rmse: 1.49858\tvalid_1's rmse: 1.5589\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[776]\ttraining's rmse: 1.48391\tvalid_1's rmse: 1.53959\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[619]\ttraining's rmse: 1.4925\tvalid_1's rmse: 1.55626\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[590]\ttraining's rmse: 1.49803\tvalid_1's rmse: 1.54398\n",
      "CV Loss =  1.555236039910202\n"
     ]
    }
   ],
   "source": [
    "#prediction_without_outliers_skfold,oof_without_outliers_skfold,feature_importance_without_outliers_skfold = non_outlier_training(df_train,df_test,train_columns,type_fold=\"SKFold\")\n",
    "prediction_without_outliers_skfold,oof_without_outliers_skfold,feature_importance_without_outliers_skfold = non_outlier_training(df_train,df_test,train_columns,type_fold=\"SKFold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_non_outlier,df_train_outlier,threshold     = create_non_outlier_data(df_train,oof_outlier_skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_without_outliers_skfold,oof_without_outliers_skfold,feature_importance_without_outliers_skfold = non_outlier_training(df_train,df_test,train_columns,type_fold=\"SKFold\")\n",
    "#prediction_outliers_reg_skfold,oof_outliers_reg_skfold,feature_importance_outliers_reg_skfold = outlier_regression_training(df_train_outlier,df_test,train_columns,type_fold=\"SKFold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_without_outliers_Rfold,oof_without_outliers_Rfold = non_outlier_training(df_train,df_test,train_columns,type_fold=\"RepeatFold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_outlier                      = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "df_test_outlier[\"target\"]            = prediction_outlier_skfold\n",
    "df_test_outlier['predicted_outlier'] = df_test_outlier['target'].map(lambda x: 1 if x>=threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_id           = df_test_outlier.loc[df_test_outlier['predicted_outlier']==1,'card_id'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_id.columns   = ['card_id']\n",
    "outlier_id           = pd.DataFrame(outlier_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test_outlier               = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "#df_test_outlier[\"target\"]     = prediction_outlier_skfold\n",
    "\n",
    "#df_test_outlier['predicted_outlier'] = df_test_outlier[\"target\"].map(lambda x: 1 if x > roc_t['threshold'].values[0] else 0 )\n",
    "#df_test_outlier['target_outlier']    = prediction_outliers_reg_skfold\n",
    "#df_test_outlier['target_non_outlier']    = prediction_without_outliers_skfold\n",
    "#df_test_outlier['final_target'] = df_test_outlier.apply(lambda x : x['target_outlier'] if x['predicted_outlier']==1 else x['target_non_outlier'],axis=1)\n",
    "#df_test_outlier.to_csv(\"submission_feature9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier_id           = pd.DataFrame(df_outlier.sort_values(by='target',ascending = False).head(25000)['card_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°0\n",
      "----------Stacking 0----------\n",
      "fold n°1\n",
      "----------Stacking 1----------\n",
      "fold n°2\n",
      "----------Stacking 2----------\n",
      "fold n°3\n",
      "----------Stacking 3----------\n",
      "fold n°4\n",
      "----------Stacking 4----------\n",
      "3.648059573748058\n"
     ]
    }
   ],
   "source": [
    "#target_non_outlier           = df_train.loc[df_train['outliers'] == 0,'target']\n",
    "prediction_stack_complete    = stacking(oof_skfold,oof_RFold,predictions_skfold,predictions_RFold,target)\n",
    "#prediction_stack_non_outlier = stacking(oof_without_outliers_skfold,oof_without_outliers_Rfold,prediction_without_outliers_skfold,prediction_without_outliers_Rfold,target_non_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output                 = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "output[\"target\"]       = predictions_skfold\n",
    "path                   = \"../result/\"\n",
    "final_name             = path+\"submission_feature22.csv\" \n",
    "output.to_csv(final_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path                   = \"../result/\"\n",
    "#final_name             = path+\"submission_stack_newhist_hist.csv\" \n",
    "#output.to_csv(final_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_without_outlier_output           = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "test_without_outlier_output[\"target\"] = prediction_without_outliers_skfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_outlier_non_outlier(\"3.694\",outlier_id,test_without_outlier_output,\"submission_3.694_outlier_auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature1 - Remove insignificant features from 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1                   = \"../result/\"\n",
    "final_name1             = path+\"3.691.csv\" \n",
    "path2                   = \"../result/\"\n",
    "final_name2             = path+\"3.695.csv\" \n",
    "df_base1 = pd.read_csv(final_name1)\n",
    "df_base2 = pd.read_csv(final_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base1 = pd.merge(df_base1,df_base2,on='card_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base1['target'] = df_base1['target_x']*0.65 + df_base1['target_y']*0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base1 = df_base1[['card_id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "path                   = \"../result/\"\n",
    "final_name             = path+\"submission_blend_3.691_3.695.csv\" \n",
    "df_base1.to_csv(final_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
