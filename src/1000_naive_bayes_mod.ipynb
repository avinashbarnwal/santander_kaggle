{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modified Naive Bayes scores 0.899 LB - Santander**\n",
    "\n",
    "In this kernel we demonstrate that unconstrained Naive Bayes can score 0.899 LB. \n",
    "\n",
    "-  I call it **unconstrained** because it doesn't assume that each variable has a Gaussian distribution like typical Naive Bayes. \n",
    "-  Instead we allow for arbitrary distributions and we plot these distributions below. \n",
    "-  I called it \"modified\" because we don't reverse the conditional probabilities.\n",
    "\n",
    "This kernel is useful because \n",
    "-  It shows that an accurate score can be achieved using a simple model that assumes the variables are independent.\n",
    "-  This kernel displays interesting EDA which provides insights about the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np, pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn  as sns\n",
    "import lightgbm as lgb\n",
    "from   sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "from   sklearn.metrics   import mean_squared_error\n",
    "import utils\n",
    "from   sklearn           import metrics\n",
    "import matplotlib\n",
    "from   woe               import WOE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 28.98it/s]\n"
     ]
    }
   ],
   "source": [
    "path        = '../data/input/input_pkl/train/'\n",
    "train    = utils.read_pickles(path)\n",
    "train0 = df_train[ df_train['target']==0 ].copy()\n",
    "train1 = df_train[ df_train['target']==1 ].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Functions\n",
    "Below are functions to calcuate various statistical things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATE MEANS AND STANDARD DEVIATIONS#\n",
    "\n",
    "s = [0]*200\n",
    "m = [0]*200\n",
    "\n",
    "for i in range(200):\n",
    "    s[i] = np.std(train['var_'+str(i)])\n",
    "    m[i] = np.mean(train['var_'+str(i)])\n",
    "    \n",
    "#CALCULATE PROB(TARGET=1 | X)\n",
    "\n",
    "def getp(i,x):\n",
    "    c = 3 #smoothing factor\n",
    "    a = len(train1[(train1['var_'+str(i)]>x-s[i]/c)&(train1['var_'+str(i)]<x+s[i]/c)]) \n",
    "    b = len(train0[(train0['var_'+str(i)]>x-s[i]/c)&(train0['var_'+str(i)]<x+s[i]/c)])\n",
    "    if a+b<500: return 0.1 #smoothing factor\n",
    "    # RETURN PROBABILITY\n",
    "    return a / (a+b)\n",
    "    # ALTERNATIVELY RETURN ODDS\n",
    "    # return a / b\n",
    "    \n",
    "# SMOOTH A DISCRETE FUNCTION\n",
    "def smooth(x,st=1):\n",
    "    for j in range(st):\n",
    "        x2 = np.ones(len(x)) * 0.1\n",
    "        for i in range(len(x)-2):\n",
    "            x2[i+1] = 0.25*x[i]+0.5*x[i+1]+0.25*x[i+2]\n",
    "        x = x2.copy()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Target Density and Target Probability  \n",
    "Below are two plots for each of the 200 variables.  \n",
    "The first is the density of target=1 versus target=0.  \n",
    "The second gives the probability that target=1 given different values for var_k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAW PLOTS, YES OR NO\n",
    "# DATA HAS Z-SCORE RANGE OF -4.5 TO 4.5\n",
    "Picture = True\n",
    "rmin    = -5; \n",
    "rmax    = 5; \n",
    "# CALCULATE PROBABILITIES FOR 501 BINS\n",
    "# STORE PROBABILITIES IN PR\n",
    "res   = 501\n",
    "pr    = 0.1 * np.ones((200,res))\n",
    "pr2   = pr.copy()\n",
    "xr    = np.zeros((200,res))\n",
    "xr2   = xr.copy()\n",
    "ct2   = 0\n",
    "for j in range(50):\n",
    "    if Picture: plt.figure(figsize=(15,8))\n",
    "    for v in range(4):\n",
    "        ct = 0\n",
    "        # CALCULATE PROBABILITY FUNCTION FOR VAR\n",
    "        for i in np.linspace(rmin,rmax,res):\n",
    "            pr[v+4*j,ct] = getp(v+4*j,m[v+4*j]+i*s[v+4*j])\n",
    "            xr[v+4*j,ct] = m[v+4*j]+i*s[v+4*j]\n",
    "            xr2[v+4*j,ct] = i\n",
    "            ct += 1\n",
    "        if Picture:\n",
    "            # SMOOTH FUNCTION FOR PRETTIER DISPLAY\n",
    "            # BUT USE UNSMOOTHED FUNCTION FOR PREDICTION\n",
    "            pr2[v+4*j,:] = smooth(pr[v+4*j,:],res//10)\n",
    "            # DISPLAY PROBABILITY FUNCTION\n",
    "            plt.subplot(2, 4, ct2%4+5)\n",
    "            plt.plot(xr[v+4*j,:],pr2[v+4*j,:],'-')\n",
    "            plt.title('P( t=1 | var_'+str(v+4*j)+' )')\n",
    "            xx = plt.xlim()\n",
    "            # DISPLAY TARGET DENSITIES\n",
    "            plt.subplot(2, 4, ct2%4+1)            \n",
    "            sns.distplot(train0['var_'+str(v+4*j)], label = 't=0')\n",
    "            sns.distplot(train1['var_'+str(v+4*j)], label = 't=1')\n",
    "            plt.title('var_'+str(v+4*j))\n",
    "            plt.legend()\n",
    "            plt.xlim(xx)\n",
    "            plt.xlabel('')\n",
    "        if (ct2%8==0): print('Showing vars',ct2,'to',ct2+7,'...')\n",
    "        ct2 += 1\n",
    "    if Picture: plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Probability Function  \n",
    "Above, the target probability function was calculated for each variable with resolution equal to standard deviation / 50 from -5 to 5.  \n",
    "For example, we know the Probability ( target=1 | var=x )  \n",
    "for z-score = -5.00, -4.98, ..., -0.02, 0, 0.02, ..., 4.98, 5.00 where z-score = (x - var_mean) / (var_standard_deviation).  \n",
    "The python function below accesses these pre-calculated values from their numpy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getp2(i,x):\n",
    "    z  = (x-m[i])/s[i]\n",
    "    ss = (rmax-rmin)/(res-1)\n",
    "    if res%2==0: idx = min( (res+1)//2 + z//ss, res-1)\n",
    "    else: idx = min( (res+1)//2 + (z-ss/2)//ss, res-1)\n",
    "    idx = max(idx,0)\n",
    "    return pr[i,int(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation\n",
    "We will ignore the training data's target and make our own prediction for each training observation. Then using our predictions and the true value, we will calculate validation AUC. (There is a leak in this validation method but none-the-less it gives an approximation of CV score. If you wish to tune this model, you should use a proper validation set. Current actual 5-fold CV is 0.8995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Calculating 200000 predictions and displaying a few examples...')\n",
    "pred = [0]*200000; ct = 0\n",
    "for r in train.index:\n",
    "    p = 0.1\n",
    "    for i in range(200):\n",
    "        p *= 10*getp2(i,train.iloc[r,2+i])\n",
    "    if ct%25000==0: print('train',r,'has target =',train.iloc[r,1],'and prediction =',p)\n",
    "    pred[ct]=p; ct += 1\n",
    "print('###############')\n",
    "print('Validation AUC =',roc_auc_score(train['target'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict Test and Submit\n",
    "Naive Bayes is a simple model. Given observation with var_0 = 15, var_1 = 5, var_2 = 10, etc. We compute the probability that target=1 by calculating P(t=1) * P(t=1 | var_0=15)/P(t=1) * P(t=1 | var_1=5)/P(t=1) * P(t=1 | var_2=10)/P(t=1) * ... where P(t=1)=0.1 and the other probabilities are computed above by counting occurences in the training data. So each observation has 200 variables and we simply multiply together the 200 target probabilities given by each variable. (In typical Naive Bayes, you use Bayes formula, reverse the probabilities, and find P(var_0=15 | t=1). This is modified Naive Bayes and more intuitive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test.csv')\n",
    "print('Calculating 200000 predictions and displaying a few examples...')\n",
    "pred = [0]*200000; ct = 0\n",
    "for r in test.index:\n",
    "    p = 0.1\n",
    "    for i in range(200):\n",
    "        p *= 10*getp2(i,test.iloc[r,1+i])\n",
    "    if ct%25000==0: print('test',r,'has prediction =',p)\n",
    "    pred[ct]=p\n",
    "    ct += 1\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub['target'] = pred\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "print('###############')\n",
    "print('Finished. Wrote predictions to submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[ sub['target']>1 , 'target'] = 1\n",
    "b = plt.hist(sub['target'], bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In conclusion we used modified Naive Bayes to predict Santander Customer transactions. Since we achieved an accurate score of 0.899 LB (which rivals other methods that capture interactions), this demonstrates that there is little or no interaction between the 200 variables. Additionally in this kernel we observed some fascinating EDA which provide insights about the variables. Can this method be improved? Perhaps by tuning this model better (adjust smoothing, resolution, etc) we can increase validation AUC and increase LB AUC but I don't think we can score over 0.902 with this method. There are other secrets hiding in the Santander data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
