{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiuybavYk97c"
   },
   "source": [
    "**Magic Explained**\n",
    "\n",
    "- Remove Fake Samples from test cases\n",
    "- combined train and test cases and Create count features for train and test\n",
    "- Create Interaction features between original and count.\n",
    "\n",
    "- Run augment LightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 371,
     "status": "error",
     "timestamp": 1555090000582,
     "user": {
      "displayName": "avinash barnwal",
      "photoUrl": "https://lh4.googleusercontent.com/-oeHlvwV-slc/AAAAAAAAAAI/AAAAAAAAAAA/Ssy6H-bsJ24/s64/photo.jpg",
      "userId": "04623478674845675981"
     },
     "user_tz": 240
    },
    "id": "hRSBWGmHk97f",
    "outputId": "72b64590-0bdf-4fae-c300-bf795dead812"
   },
   "outputs": [],
   "source": [
    "import numpy  as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from   tqdm   import tqdm_notebook as tqdm\n",
    "import random\n",
    "import os,sys,math\n",
    "from   sklearn.model_selection import KFold, StratifiedKFold\n",
    "from   sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lon600Qwk97q"
   },
   "outputs": [],
   "source": [
    "def set_seed(number):\n",
    "    random.seed(number)\n",
    "    np.random.seed(number)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0tl1FH7k97t"
   },
   "outputs": [],
   "source": [
    "set_seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PlOsY6Hk97z"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    train_path        = '../../data/input/input_pkl/train/'\n",
    "    train             = utils.read_pickles(train_path)\n",
    "    test_path         = '../../data/input/input_pkl/test/'\n",
    "    test              = utils.read_pickles(test_path)\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gDAclHwk974",
    "outputId": "b749d13c-1ba7-46ea-deea-999b2fe561a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 15.83it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 21.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train,test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNQ84lYck98B"
   },
   "outputs": [],
   "source": [
    "def getUniquesamples(test):\n",
    "    \n",
    "    test.drop(['ID_code'], axis=1, inplace=True)\n",
    "    test           = test.values\n",
    "    \n",
    "    unique_samples = []\n",
    "    unique_count   = np.zeros_like(test)\n",
    "    \n",
    "    for feature in tqdm(range(test.shape[1])):\n",
    "        _, index_, count_ = np.unique(test[:, feature], return_counts=True, return_index=True)\n",
    "        unique_count[index_[count_ == 1], feature] += 1\n",
    "        \n",
    "    # Samples which have unique values are real the others are fake\n",
    "    real_samples_indexes      = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "    synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "    \n",
    "    return real_samples_indexes,synthetic_samples_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iR_D-qwk98G",
    "outputId": "bd7e225e-a157-4369-e623-c7d62e9284a6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d407fbf3065d464b80ef0c5c62621a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "real_samples_indexes,synthetic_samples_indexes = getUniquesamples(test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwV4cNiJk98L"
   },
   "outputs": [],
   "source": [
    "def findmeancounts(bins,values,x):\n",
    "    \n",
    "    for i in range(len(bins)):\n",
    "        \n",
    "        if x < bins[i]:\n",
    "            return values[i]\n",
    "        \n",
    "    return values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGEagNMKk98P"
   },
   "outputs": [],
   "source": [
    "def FE1(train,test): \n",
    "    \n",
    "    feats = [\"var_\"+str(i) for i in range(200)] \n",
    "    df    = pd.concat([train,test.ix[real_samples_indexes]])\n",
    "    \n",
    "    for feat in feats:\n",
    "        \n",
    "        temp                = df[feat].value_counts(dropna = True)\n",
    "        \n",
    "        #Clamping the unique counts \n",
    "        train[feat+\"vc\"] = train[feat].map(temp).map(lambda x:min(10,x)).astype(np.uint8)\n",
    "        test[feat+\"vc\"]  = test[feat].map(temp).map(lambda x:min(10,x)).astype(np.uint8)\n",
    "        \n",
    "        #Create interaction between Feature and Count Feature based on Count 1\n",
    "        train[feat+\"sum\"]   = ((train[feat] - df[feat].mean())*train[feat+\"vc\"].map(lambda x:int(x>1))).astype(np.float32)\n",
    "        test[feat+\"sum\"]    = ((test[feat] - df[feat].mean())*test[feat+\"vc\"].map(lambda x:int(x>1))).astype(np.float32)\n",
    "        \n",
    "        #Create interaction between Feature and Count Feature based on Count 2\n",
    "        train[feat+\"sum2\"]  = ((train[feat])*train[feat+\"vc\"].map(lambda x:int(x>2))).astype(np.float32)\n",
    "        test[feat+\"sum2\"]   = ((test[feat])*test[feat+\"vc\"].map(lambda x:int(x>2))).astype(np.float32)\n",
    "        \n",
    "        #Create interaction between Feature and Count Feature based on Count 4\n",
    "        train[feat+\"sum3\"] = ((train[feat])*train[feat+\"vc\"].map(lambda x:int(x>4))).astype(np.float32) \n",
    "        test[feat+\"sum3\"] = ((test[feat])*test[feat+\"vc\"].map(lambda x:int(x>4))).astype(np.float32) \n",
    "        \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rVuYh84k98V"
   },
   "outputs": [],
   "source": [
    "train,test = FE1(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-u7rJrWk98Z"
   },
   "outputs": [],
   "source": [
    "def augment(x,y,t=2):\n",
    "    \n",
    "    xs,xn = [],[]\n",
    "    \n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        featnum = x1.shape[1]//200 - 1\n",
    "        for c in range(200):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,[c] + [200 + featnum * c + idc for idc in range(featnum)]] = x1[ids][:,[c] + [200 + featnum * c + idc for idc in range(featnum)]]\n",
    "        xn.append(x1)\n",
    "\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        featnum = x1.shape[1]//200 - 1\n",
    "        for c in range(200):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,[c] + [200 + featnum * c + idc for idc in range(1)]] = x1[ids][:,[c] + [200 + featnum * c + idc for idc in range(1)]]\n",
    "        xs.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x  = np.vstack([x,xs,xn])\n",
    "    y  = np.concatenate([y,ys,yn])\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0UjsKYqk98c"
   },
   "outputs": [],
   "source": [
    "features = [col for col in train.columns if col not in ['target', 'ID_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zH9fLV-Yk98h"
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "                \"objective\" : \"binary\",\n",
    "                \"metric\" : \"auc\",\n",
    "                \"boosting\": 'gbdt',\n",
    "                \"max_depth\" : -1,\n",
    "                \"num_leaves\" : 15,\n",
    "                \"learning_rate\" : 0.01,\n",
    "                \"bagging_freq\": 5,\n",
    "                \"bagging_fraction\" : 0.6,\n",
    "                \"feature_fraction\" : 0.05,\n",
    "                \"min_data_in_leaf\": 50,\n",
    "                \"min_sum_heassian_in_leaf\": 10,\n",
    "                \"tree_learner\": \"serial\",\n",
    "                \"boost_from_average\": \"false\",\n",
    "                \"lambda_l1\" : 1.,\n",
    "                #     \"lambda_l2\" : 0.5,\n",
    "                \"bagging_seed\" : 2007,\n",
    "                \"verbosity\" : 1,\n",
    "                \"seed\": 2007\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9kjpIhBk98l"
   },
   "outputs": [],
   "source": [
    "skf                   = StratifiedKFold(n_splits=5, shuffle=True, random_state=2007)\n",
    "oof                   = train[['ID_code', 'target']]\n",
    "oof['predict']        = 0\n",
    "predictions           = np.zeros((test.shape[0],5))\n",
    "val_aucs              = []\n",
    "feature_importance_df = pd.DataFrame()\n",
    "features              = [col for col in train.columns if col not in ['target', 'ID_code']]\n",
    "X_test                = test[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn2ILZKEk98s",
    "outputId": "f9f521c9-06b4-421f-8cfc-d01fc4f5ee9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[1000]\ttraining's auc: 0.952668\tvalid_1's auc: 0.856333\n",
      "[2000]\ttraining's auc: 0.963747\tvalid_1's auc: 0.878537\n",
      "[3000]\ttraining's auc: 0.96875\tvalid_1's auc: 0.887747\n",
      "[4000]\ttraining's auc: 0.971947\tvalid_1's auc: 0.892256\n",
      "[5000]\ttraining's auc: 0.974386\tvalid_1's auc: 0.894737\n",
      "[6000]\ttraining's auc: 0.97655\tvalid_1's auc: 0.89612\n",
      "[7000]\ttraining's auc: 0.978601\tvalid_1's auc: 0.896826\n",
      "[8000]\ttraining's auc: 0.980573\tvalid_1's auc: 0.897369\n",
      "[9000]\ttraining's auc: 0.982471\tvalid_1's auc: 0.897492\n",
      "[10000]\ttraining's auc: 0.984256\tvalid_1's auc: 0.897561\n",
      "[11000]\ttraining's auc: 0.985935\tvalid_1's auc: 0.897637\n",
      "[12000]\ttraining's auc: 0.987467\tvalid_1's auc: 0.897621\n",
      "[13000]\ttraining's auc: 0.988885\tvalid_1's auc: 0.897577\n",
      "[14000]\ttraining's auc: 0.990176\tvalid_1's auc: 0.897495\n",
      "[15000]\ttraining's auc: 0.991355\tvalid_1's auc: 0.897447\n",
      "Early stopping, best iteration is:\n",
      "[10925]\ttraining's auc: 0.985813\tvalid_1's auc: 0.897675\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[1000]\ttraining's auc: 0.95244\tvalid_1's auc: 0.85948\n",
      "[2000]\ttraining's auc: 0.963535\tvalid_1's auc: 0.880979\n",
      "[3000]\ttraining's auc: 0.968641\tvalid_1's auc: 0.88987\n",
      "[4000]\ttraining's auc: 0.971903\tvalid_1's auc: 0.894353\n",
      "[5000]\ttraining's auc: 0.97435\tvalid_1's auc: 0.896844\n",
      "[6000]\ttraining's auc: 0.976485\tvalid_1's auc: 0.898009\n",
      "[7000]\ttraining's auc: 0.978463\tvalid_1's auc: 0.898677\n",
      "[8000]\ttraining's auc: 0.980448\tvalid_1's auc: 0.898897\n",
      "[9000]\ttraining's auc: 0.982341\tvalid_1's auc: 0.899019\n",
      "[10000]\ttraining's auc: 0.984138\tvalid_1's auc: 0.899082\n",
      "[11000]\ttraining's auc: 0.985826\tvalid_1's auc: 0.898997\n",
      "[12000]\ttraining's auc: 0.98737\tvalid_1's auc: 0.898961\n",
      "[13000]\ttraining's auc: 0.988817\tvalid_1's auc: 0.898981\n",
      "[14000]\ttraining's auc: 0.990121\tvalid_1's auc: 0.898949\n",
      "[15000]\ttraining's auc: 0.991295\tvalid_1's auc: 0.898838\n",
      "Early stopping, best iteration is:\n",
      "[10042]\ttraining's auc: 0.98421\tvalid_1's auc: 0.899093\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[1000]\ttraining's auc: 0.952385\tvalid_1's auc: 0.862838\n",
      "[2000]\ttraining's auc: 0.963519\tvalid_1's auc: 0.882627\n",
      "[3000]\ttraining's auc: 0.968703\tvalid_1's auc: 0.890934\n",
      "[4000]\ttraining's auc: 0.971959\tvalid_1's auc: 0.895014\n",
      "[5000]\ttraining's auc: 0.974456\tvalid_1's auc: 0.897374\n",
      "[6000]\ttraining's auc: 0.976606\tvalid_1's auc: 0.898348\n",
      "[7000]\ttraining's auc: 0.978628\tvalid_1's auc: 0.898824\n",
      "[8000]\ttraining's auc: 0.980616\tvalid_1's auc: 0.899017\n",
      "[9000]\ttraining's auc: 0.982516\tvalid_1's auc: 0.899072\n",
      "[10000]\ttraining's auc: 0.984307\tvalid_1's auc: 0.899129\n",
      "[11000]\ttraining's auc: 0.985973\tvalid_1's auc: 0.899204\n",
      "[12000]\ttraining's auc: 0.987529\tvalid_1's auc: 0.899143\n",
      "[13000]\ttraining's auc: 0.988943\tvalid_1's auc: 0.899148\n",
      "[14000]\ttraining's auc: 0.990224\tvalid_1's auc: 0.899057\n",
      "[15000]\ttraining's auc: 0.991403\tvalid_1's auc: 0.89905\n",
      "[16000]\ttraining's auc: 0.992443\tvalid_1's auc: 0.898953\n",
      "Early stopping, best iteration is:\n",
      "[11046]\ttraining's auc: 0.986048\tvalid_1's auc: 0.899236\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[1000]\ttraining's auc: 0.95227\tvalid_1's auc: 0.861892\n",
      "[2000]\ttraining's auc: 0.963416\tvalid_1's auc: 0.882623\n",
      "[3000]\ttraining's auc: 0.968589\tvalid_1's auc: 0.891238\n",
      "[4000]\ttraining's auc: 0.971782\tvalid_1's auc: 0.895622\n",
      "[5000]\ttraining's auc: 0.974216\tvalid_1's auc: 0.898032\n",
      "[6000]\ttraining's auc: 0.976335\tvalid_1's auc: 0.899291\n",
      "[7000]\ttraining's auc: 0.978374\tvalid_1's auc: 0.899853\n",
      "[8000]\ttraining's auc: 0.980366\tvalid_1's auc: 0.900232\n",
      "[9000]\ttraining's auc: 0.982249\tvalid_1's auc: 0.90042\n",
      "[10000]\ttraining's auc: 0.984037\tvalid_1's auc: 0.900455\n",
      "[11000]\ttraining's auc: 0.985705\tvalid_1's auc: 0.900418\n",
      "[12000]\ttraining's auc: 0.987253\tvalid_1's auc: 0.900515\n",
      "[13000]\ttraining's auc: 0.988683\tvalid_1's auc: 0.900533\n",
      "[14000]\ttraining's auc: 0.989974\tvalid_1's auc: 0.900504\n",
      "[15000]\ttraining's auc: 0.991172\tvalid_1's auc: 0.900501\n",
      "[16000]\ttraining's auc: 0.992267\tvalid_1's auc: 0.900494\n",
      "[17000]\ttraining's auc: 0.993263\tvalid_1's auc: 0.90054\n",
      "Early stopping, best iteration is:\n",
      "[12625]\ttraining's auc: 0.988165\tvalid_1's auc: 0.900577\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[1000]\ttraining's auc: 0.952214\tvalid_1's auc: 0.860511\n",
      "[2000]\ttraining's auc: 0.963447\tvalid_1's auc: 0.882592\n",
      "[3000]\ttraining's auc: 0.968459\tvalid_1's auc: 0.891876\n",
      "[4000]\ttraining's auc: 0.971678\tvalid_1's auc: 0.896868\n",
      "[5000]\ttraining's auc: 0.974125\tvalid_1's auc: 0.899384\n",
      "[6000]\ttraining's auc: 0.976273\tvalid_1's auc: 0.900668\n",
      "[7000]\ttraining's auc: 0.978318\tvalid_1's auc: 0.90135\n",
      "[8000]\ttraining's auc: 0.980278\tvalid_1's auc: 0.90164\n",
      "[9000]\ttraining's auc: 0.982181\tvalid_1's auc: 0.901841\n",
      "[10000]\ttraining's auc: 0.983984\tvalid_1's auc: 0.901954\n",
      "[11000]\ttraining's auc: 0.985649\tvalid_1's auc: 0.902067\n",
      "[12000]\ttraining's auc: 0.987224\tvalid_1's auc: 0.902088\n",
      "[13000]\ttraining's auc: 0.988669\tvalid_1's auc: 0.902129\n",
      "[14000]\ttraining's auc: 0.989982\tvalid_1's auc: 0.902077\n",
      "[15000]\ttraining's auc: 0.991169\tvalid_1's auc: 0.90202\n",
      "[16000]\ttraining's auc: 0.992262\tvalid_1's auc: 0.901949\n",
      "[17000]\ttraining's auc: 0.993247\tvalid_1's auc: 0.901955\n",
      "[18000]\ttraining's auc: 0.994117\tvalid_1's auc: 0.901992\n",
      "Early stopping, best iteration is:\n",
      "[13019]\ttraining's auc: 0.988697\tvalid_1's auc: 0.902146\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(train, train['target'])):\n",
    "    \n",
    "    X_train, y_train = train.iloc[trn_idx][features], train.iloc[trn_idx]['target']\n",
    "    X_valid, y_valid = train.iloc[val_idx][features], train.iloc[val_idx]['target']\n",
    "    \n",
    "    N                = 1\n",
    "    p_valid,yp       = 0,0\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        X_t, y_t                   = augment(X_train.values, y_train.values)\n",
    "        weights                    = np.array([0.8] * X_t.shape[0])\n",
    "        weights[:X_train.shape[0]] = 1.0\n",
    "        trn_data                   = lgb.Dataset(X_t, label=y_t, weight = weights)\n",
    "        val_data                   = lgb.Dataset(X_valid, label=y_valid)\n",
    "        evals_result               = {}\n",
    "        lgb_clf                    = lgb.train(lgb_params,\n",
    "                                               trn_data,\n",
    "                                               100000,\n",
    "                                               valid_sets            = [trn_data, val_data],\n",
    "                                               early_stopping_rounds = 5000,\n",
    "                                               verbose_eval          = 1000,\n",
    "                                               evals_result          = evals_result)\n",
    "        \n",
    "        p_valid                    += lgb_clf.predict(X_valid)\n",
    "        yp                         += lgb_clf.predict(X_test)\n",
    "        \n",
    "    fold_importance_df                  = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"]       = features\n",
    "    fold_importance_df[\"importance\"]    = lgb_clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"]          = fold + 1\n",
    "    feature_importance_df               = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    oof['predict'][val_idx]             = p_valid/N\n",
    "    \n",
    "    val_score = roc_auc_score(y_valid, p_valid)\n",
    "    val_aucs.append(val_score)\n",
    "    \n",
    "    predictions[:,fold] = yp/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5QU8w5iNk98x"
   },
   "outputs": [],
   "source": [
    "mean_auc = np.mean(val_aucs)\n",
    "std_auc = np.std(val_aucs)\n",
    "all_auc = roc_auc_score(oof['target'], oof['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0OMki5Rk981"
   },
   "outputs": [],
   "source": [
    "print(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqYL2jgRk987"
   },
   "outputs": [],
   "source": [
    "cols          = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "                 .groupby(\"feature\")\n",
    "                 .mean()\n",
    "                 .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGSQGS-Mk99B"
   },
   "outputs": [],
   "source": [
    "##submission##\n",
    "sub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\n",
    "sub_df[\"target\"] = np.mean(predictions,axis = 1)\n",
    "sub_df.to_csv(\"lgb_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sol1_0.923_N_5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
