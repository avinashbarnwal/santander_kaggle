{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A proof of synthetic data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv', index_col=0)\n",
    "test = pd.read_csv('../input/test.csv', index_col=0)\n",
    "\n",
    "target = train.target.values\n",
    "train.drop('target', axis=1, inplace=True)\n",
    "train.shape, target.shape, test.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = (target == 1)\n",
    "neg_idx = (target == 0)\n",
    "stats = []\n",
    "for col in train.columns:\n",
    "    stats.append([\n",
    "        train.loc[pos_idx, col].mean(),\n",
    "        train.loc[pos_idx, col].std(),\n",
    "        train.loc[neg_idx, col].mean(),\n",
    "        train.loc[neg_idx, col].std()\n",
    "    ])\n",
    "    \n",
    "stats_df = pd.DataFrame(stats, columns=['pos_mean', 'pos_sd', 'neg_mean', 'neg_sd'])\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npos = pos_idx.sum()\n",
    "nneg = neg_idx.sum()\n",
    "\n",
    "sim_feats = []\n",
    "for pos_mean, pos_sd, neg_mean, neg_sd in stats:\n",
    "    pos_feat = np.random.normal(loc=pos_mean, scale=pos_sd, size=npos)\n",
    "    neg_feat = np.random.normal(loc=neg_mean, scale=neg_sd, size=nneg)\n",
    "    sim_feats.append(np.hstack([pos_feat, neg_feat]))\n",
    "    \n",
    "sim_feats = np.column_stack(sim_feats)\n",
    "sim_target = np.hstack([np.ones(npos), np.zeros(nneg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_feats.shape, sim_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.335,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.041,\n",
    "    'learning_rate': 0.0083,\n",
    "    'max_depth': -1,\n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary', \n",
    "    'verbosity': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = lgb.Dataset(sim_feats, sim_target)\n",
    "cv = lgb.cv(param, trn_data, 100000, shuffle=True, early_stopping_rounds=600, verbose_eval=600)\n",
    "print(cv['auc-mean'][-1], len(cv['auc-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can achieve 0.885 just using train data's mean and sd, this is not bad! Maybe the data is generated using this way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate probability use hypothetical test\n",
    "If each feature is generated by sample positive samples and negtive samples, then we can use hypothetical test to distinguish them. The positive samples and negative samples of each feature are slightly different. Let's take var_0 and var_1 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "# var_0\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.distplot(train.loc[pos_idx, 'var_0'], hist=False, label='pos', color='blue')\n",
    "sns.distplot(train.loc[neg_idx, 'var_0'], hist=False, label='neg', color='orange')\n",
    "plt.vlines(x=[stats_df.loc[0, 'pos_mean'], stats_df.loc[0, 'neg_mean']], ymin=0, ymax=0.15, colors=['blue', 'orange'])\n",
    "plt.xlabel('var_0')\n",
    "plt.title('Real data')\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.distplot(sim_feats[pos_idx, 0], hist=False, label='pos', color='blue')\n",
    "sns.distplot(sim_feats[neg_idx, 0], hist=False, label='neg', color='orange')\n",
    "plt.vlines(x=[stats_df.loc[0, 'pos_mean'], stats_df.loc[0, 'neg_mean']], ymin=0, ymax=0.15, colors=['blue', 'orange'])\n",
    "plt.title('Simulated data')\n",
    "plt.legend()\n",
    "plt.xlabel('var_0')\n",
    "\n",
    "# var_1\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.distplot(train.loc[pos_idx, 'var_1'], hist=False, label='pos', color='blue')\n",
    "sns.distplot(train.loc[neg_idx, 'var_1'], hist=False, label='neg', color='orange')\n",
    "plt.vlines(x=[stats_df.loc[1, 'pos_mean'], stats_df.loc[1, 'neg_mean']], ymin=0, ymax=0.15, colors=['blue', 'orange'])\n",
    "plt.xlabel('var_1')\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.distplot(sim_feats[pos_idx, 1], hist=False, label='pos', color='blue')\n",
    "sns.distplot(sim_feats[neg_idx, 1], hist=False, label='neg', color='orange')\n",
    "plt.vlines(x=[stats_df.loc[1, 'pos_mean'], stats_df.loc[1, 'neg_mean']], ymin=0, ymax=0.15, colors=['blue', 'orange'])\n",
    "plt.legend()\n",
    "plt.xlabel('var_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zval1 = (train.values - stats_df.neg_mean.values) / stats_df.neg_sd.values\n",
    "zval1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval1 = (1 - norm.cdf(np.abs(zval1))) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Since we have 200 feats, we get 200 pvalue for each sample, we can multiply them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1 = pval1.prod(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The smaller the prob1, the more likely a sample is positive. let's see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(target, 1/prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we test whether a sample is positive, we can get another hypothetical test:\n",
    "\n",
    "Null hypothesis: a sample is positive(target == 1)\n",
    "Alternative hypothesis: a sample is not positive(target == 0)\n",
    "If we get a small pvalue(< 0.05), we reject the null hypothesis, i.e. the bigger the pvalue, the more likely a sample is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zval2 = (train.values - stats_df.pos_mean.values) / stats_df.pos_sd.values\n",
    "pval2 = (1 - norm.cdf(np.abs(zval2))) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2 = pval2.prod(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(target, prob2 / prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can get 0.874 just using Probability theory, It's quite good I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_zval1 = (test.values - stats_df.neg_mean.values) / stats_df.neg_sd.values\n",
    "te_pval1 = (1 - norm.cdf(np.abs(te_zval1))) * 2\n",
    "te_prob1 = te_pval1.prod(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_zval2 = (test.values - stats_df.pos_mean.values) / stats_df.pos_sd.values\n",
    "te_pval2 = (1 - norm.cdf(np.abs(te_zval2))) * 2\n",
    "te_prob2 = te_pval2.prod(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = te_prob2 / te_prob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'ID_code': test.index,\n",
    "    'target': pred\n",
    "}).to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conclusion\n",
    "Branden Murray's hypothesis For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together is a wonderful explanation of shuffling also works and weak interaction between features.\n",
    "\n",
    "We can even use tranditional Probability theory to calculate the P(target==1) value to achive 0.874 local cv. But this model is still too naive, the feature is not normal distribution(I try normality test, none of the 200 features passed), and the positive samples and negative samples is not variance homogeneity(2/3 of the features failed variance homogeneity test).\n",
    "\n",
    "Hope this kernal can help, thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
